# ExplainableMLOntology
Work related to the Article Semantic Description of Explainable Machine Learning Workflows for Improving Trust

Abstract: Explainable Machine Learning comprises methods and techniques that enable users to better understand the machine learning functioning and results. This work proposes an ontology that represents explainable machine learning experiments, allowing data scientists and developers to have a holistic view and better understanding of the explainable machine learning process and trust building. We developed the ontology by reusing an existing domain-specific ontology (ML-SCHEMA) and grounding it in the Unified Foundational Ontology (UFO), aiming at in-teroperability. The proposed ontology is structured in three modules: (1) the general module, (2) the specific module, and (3) the explanation module. The ontology was evaluated using a case study in the scenario of the COVID-19 pandemic using healthcare data from patients, which is sensitive data. In the case study, we trained a Support Vector Machine to predict mortality of patients infected with COVID-19 and applied existing explanation methods to generate explana-tions from the trained model. Based on the case study, we populated the ontology and queried it to ensure that the ontology fulfills its intended purpose, demonstrating its suitability.

Authors: Patricia Inoue Nakagawa, Luís Ferreira Pires, João Luiz Rebelo Moreira, Luiz Olavo Bonino, and Faiza Bukhsh

Folders:
  Data and Code: datasets and notebook for the Covid-19 and SVM case study
  
  Visual Paradigm conceptual models: Conceptual models developed using Visual Paradigm and OntoUML Plugin (available in https://github.com/OntoUML/ontouml-vp-plugin)
  
  Protege_ontologies: Files in ttl exported from Visual Paradigm and can be opened in Protégé and saved as OWL. 
